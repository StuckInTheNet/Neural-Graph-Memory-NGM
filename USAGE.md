# Neural Graph Memory (NGM) - Usage Guide

This repository contains a working implementation of Neural Graph Memory, a biologically-inspired graph-based memory architecture for AI systems.

## ðŸš€ Quick Start

### Installation

```bash
pip install -r requirements.txt
```

### Basic Usage

```python
from SRC.memory import NeuralGraphMemory
from SRC.load_data import load_graph_data

# Initialize NGM
ngm = NeuralGraphMemory()

# Add a memory node
text_embedding = ngm.embed_text("I saw a golden retriever in the park")
ngm.add_memory_node("memory_1", "text", "Saw a golden retriever", text_embedding)

# Query the memory
answer = ngm.query("What did I see in the park?")
print(answer)  # "Saw a golden retriever"
```

### Loading Data

```python
# Load synthetic data
data = load_graph_data('data')
ngm.add_nodes(data['nodes'])
ngm.add_edges(data['edges'])

# Query the loaded memory graph
answer = ngm.query("What was the reminder about?")
```

## ðŸ§ª Testing

Run the test suite to verify functionality:

```bash
python3 test_fixes.py
```

Run the benchmark:

```bash
python3 scripts/benchmark_ngm.py
```

## ðŸ”§ Key Features

### âœ… Fixed Issues

1. **Complete API**: All missing methods (`add_nodes`, `add_edges`, `query`) implemented
2. **Temporal Weighting**: Edges weighted based on temporal proximity
3. **Graph Traversal**: Context-aware retrieval through graph connections
4. **Multimodal Fusion**: Enhanced embedding fusion for different modalities
5. **Working Benchmark**: Functional evaluation script

### ðŸ†• New Capabilities

- **Hebbian-style Updates**: Memory strengthening through reactivation
- **Temporal Anchoring**: Time-aware edge weights with exponential decay
- **Modality Enhancement**: Specialized processing for image, audio, and event nodes
- **Graph-based Retrieval**: Attention-weighted traversal with degree centrality boosting

## ðŸ“Š Performance

The benchmark achieves:
- **100% accuracy** on synthetic queries
- **~0.02 seconds** average retrieval time
- **Graph traversal** with contextual expansion

## ðŸ”¬ Architecture Overview

```
Query â†’ Text Embedding â†’ Graph Traversal â†’ Context Expansion â†’ Response
         â†‘                    â†“
    CLIP Model          Memory Graph
                       (Nodes + Edges)
```

### Node Types
- **Text**: Natural language descriptions
- **Image**: Visual content (via CLIP)
- **Audio**: Audio transcriptions
- **Event**: Temporal episodes

### Edge Types  
- **Temporal**: Time-based connections
- **Semantic**: Content similarity
- **Learned**: Association-based links

## ðŸŽ¯ Next Steps

To fully align with the paper specifications:

1. **Gated Transformer Fusion**: Replace simple averaging with attention-based multimodal fusion
2. **Automated Node Creation**: Add change-point detection for autonomous memory formation  
3. **Ontological Constraints**: Implement typed node validation
4. **Full Hebbian Learning**: Add Î±-decay memory updates with contextual re-entry

## ðŸ“š Paper Reference

This implementation is based on:
> "Neural Graph Memory: A Structured Approach to Long-Term Memory in Multimodal Agents" by Matthew Fisher

For the complete theoretical foundation, see the paper in the `paper/` directory.