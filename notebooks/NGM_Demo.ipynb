{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "249939f5",
   "metadata": {},
   "source": [
    "# Neural Graph Memory Demo\n",
    "\n",
    "This notebook demonstrates the capabilities of **Neural Graph Memory (NGM)** ‚Äî a biologically-inspired graph-based memory architecture designed for long-term, multimodal retrieval in AI agents.\n",
    "\n",
    "The notebook includes:\n",
    "- Image and text embedding\n",
    "- Graph-based memory construction\n",
    "- Traversal and retrieval demos\n",
    "- Multimodal query-response examples\n",
    "\n",
    "All supporting assets are pulled automatically from the public GitHub repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1486382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ab216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load CLIP model and processor\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf54705",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and preprocess the image\n",
    "image_url = \"https://raw.githubusercontent.com/StuckInTheNet/neural-graph-memory-Work-In-Progress-/main/assets/architecture.png\"\n",
    "response = requests.get(image_url)\n",
    "image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "inputs = clip_processor(images=image, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f347cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract CLIP embedding\n",
    "with torch.no_grad():\n",
    "    image_features = clip_model.get_image_features(**inputs)\n",
    "\n",
    "# Normalize the embedding\n",
    "image_embedding = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "image_embedding_np = image_embedding.squeeze().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4027567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add embedding to memory graph\n",
    "ngm.add_node(\n",
    "    node_id=\"visual_architecture_memory\",\n",
    "    modality=\"image\",\n",
    "    content=\"Neural Graph Memory architecture diagram\",\n",
    "    embedding=image_embedding_np.tolist()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0a9f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if needed\n",
    "# !pip install torch torchvision transformers sentence-transformers networkx matplotlib Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3965a194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e852ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryNode:\n",
    "    def __init__(self, id, embedding, modality, metadata=None):\n",
    "        self.id = id\n",
    "        self.embedding = embedding\n",
    "        self.modality = modality\n",
    "        self.metadata = metadata or {}\n",
    "\n",
    "class NeuralGraphMemory:\n",
    "    def __init__(self):\n",
    "        self.graph = nx.Graph()\n",
    "        self.text_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.image_encoder = models.resnet18(pretrained=True)\n",
    "        self.image_encoder.fc = torch.nn.Identity()\n",
    "        self.image_encoder.eval()\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize(256), T.CenterCrop(224),\n",
    "            T.ToTensor(), T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                      std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def add_text_node(self, text):\n",
    "        embedding = self.text_encoder.encode(text)\n",
    "        return self._add_node(embedding, 'text', {\"text\": text})\n",
    "\n",
    "    def add_image_node(self, img_url):\n",
    "        image = Image.open(BytesIO(requests.get(img_url).content)).convert(\"RGB\")\n",
    "        tensor = self.transform(image).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            embedding = self.image_encoder(tensor).squeeze().numpy()\n",
    "        return self._add_node(embedding, 'image', {\"url\": img_url})\n",
    "\n",
    "    def _add_node(self, embedding, modality, metadata):\n",
    "        node_id = len(self.graph)\n",
    "        node = MemoryNode(node_id, embedding, modality, metadata)\n",
    "        self.graph.add_node(node_id, data=node)\n",
    "        if node_id > 0:\n",
    "            self.graph.add_edge(node_id - 1, node_id, relation=\"temporal\")\n",
    "        return node_id\n",
    "\n",
    "    def retrieve_nearest(self, query, top_k=3):\n",
    "        query_emb = self.text_encoder.encode(query)\n",
    "        similarities = []\n",
    "        for nid, data in self.graph.nodes(data=\"data\"):\n",
    "            sim = np.dot(data.embedding, query_emb) / (\n",
    "                np.linalg.norm(data.embedding) * np.linalg.norm(query_emb))\n",
    "            similarities.append((nid, sim))\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_k]\n",
    "\n",
    "    def traverse_temporal(self, start_id=0):\n",
    "        return list(nx.dfs_preorder_nodes(self.graph, source=start_id))\n",
    "\n",
    "    def visualize(self):\n",
    "        labels = nx.get_node_attributes(self.graph, \"data\")\n",
    "        pos = nx.spring_layout(self.graph)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        nx.draw(self.graph, pos, with_labels=True, node_color='lightblue')\n",
    "        node_texts = {k: (v.metadata.get(\"text\") or \"img\")[:15] for k, v in labels.items()}\n",
    "        nx.draw_networkx_labels(self.graph, pos, labels=node_texts, font_size=8)\n",
    "        plt.title(\"üß† Neural Graph Memory\")\n",
    "        plt.show()\n",
    "\n",
    "    def save(self, path='ngm.pkl'):\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self.graph, f)\n",
    "\n",
    "    def load(self, path='ngm.pkl'):\n",
    "        with open(path, 'rb') as f:\n",
    "            self.graph = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c36049",
   "metadata": {},
   "source": [
    "## üîß Memory Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5cd79c",
   "metadata": {},
   "source": [
    "### Adding a Visual Memory Node\n",
    "We now demonstrate how an AI agent might ingest a visual memory (e.g., a memory architecture diagram) into the graph memory system using CLIP embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad59317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Load your architecture image as an example of multimodal input\n",
    "image_url = 'https://raw.githubusercontent.com/StuckInTheNet/neural-graph-memory/main/assets/architecture.png'\n",
    "response = requests.get(image_url)\n",
    "image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e2213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngm.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85ff968",
   "metadata": {},
   "source": [
    "## üîç Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9696ad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What happened with the dog?\"\n",
    "nearest = ngm.retrieve_nearest(query)\n",
    "print(f\"Query: {query}\\n\\nTop Matches:\")\n",
    "for nid, score in nearest:\n",
    "    node = ngm.graph.nodes[nid][\"data\"]\n",
    "    print(f\" - Node {nid} ({node.modality}): {node.metadata} [Score: {score:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f199ce",
   "metadata": {},
   "source": [
    "## üîÑ Temporal Traversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10747f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = ngm.traverse_temporal()\n",
    "print(\"Traversal order:\")\n",
    "for nid in order:\n",
    "    node = ngm.graph.nodes[nid][\"data\"]\n",
    "    print(f\" - Node {nid}: {node.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53431d56",
   "metadata": {},
   "source": [
    "## üíæ Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb272cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngm.save(\"ngm_demo.pkl\")\n",
    "print(\"‚úÖ Saved memory graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aa4d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngm2 = NeuralGraphMemory()\n",
    "ngm2.load(\"ngm_demo.pkl\")\n",
    "print(\"‚úÖ Loaded memory graph with\", len(ngm2.graph), \"nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede77d8a",
   "metadata": {},
   "source": [
    "## üß† Multimodal Memory Demonstrations\n",
    "This section simulates how an AI agent stores, embeds, and retrieves rich multimodal memories.\n",
    "Each example below demonstrates a different modality or modality combination processed into graph memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b17c300",
   "metadata": {},
   "source": [
    "### üêï Image + Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18b680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and display multimodal memory image\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/9/91/Golden_Retriever_Carlos_%2810581910556%29.jpg\"\n",
    "response = requests.get(image_url)\n",
    "image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "image.show()\n",
    "\n",
    "# Corresponding caption for visual memory\n",
    "caption = \"Saw a golden retriever chasing a ball in Central Park.\"\n",
    "print(f\"Caption: {caption}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a457728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess and embed image\n",
    "inputs_image = clip_processor(images=image, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    image_features = clip_model.get_image_features(**inputs_image)\n",
    "image_embedding = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "# Preprocess and embed caption\n",
    "inputs_text = clip_processor(text=[caption], return_tensors=\"pt\", padding=True)\n",
    "with torch.no_grad():\n",
    "    text_features = clip_model.get_text_features(**inputs_text)\n",
    "text_embedding = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "# Fuse embeddings\n",
    "fused_embedding = (image_embedding + text_embedding) / 2\n",
    "fused_embedding = fused_embedding.squeeze().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207180c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Store the multimodal memory node\n",
    "ngm.add_node(\n",
    "    node_id=\"memory_golden_retriever_central_park\",\n",
    "    modality=\"image+text\",\n",
    "    content=caption,\n",
    "    embedding=fused_embedding.tolist()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0057d1a0",
   "metadata": {},
   "source": [
    "### üß† Text + Audio (Simulated Voice Note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4323b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated transcription of a voice note\n",
    "voice_note = 'Remember to review the memory architecture notes after today‚Äôs meeting.'\n",
    "print(f\"Transcribed voice note: {voice_note}\")\n",
    "\n",
    "# Embed and normalize\n",
    "inputs_text = clip_processor(text=[voice_note], return_tensors=\"pt\", padding=True)\n",
    "with torch.no_grad():\n",
    "    text_features = clip_model.get_text_features(**inputs_text)\n",
    "text_embedding = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "ngm.add_node(\n",
    "    node_id=\"memory_audio_text_meeting_review\",\n",
    "    modality=\"text\",\n",
    "    content=voice_note,\n",
    "    embedding=text_embedding.squeeze().tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7430d292",
   "metadata": {},
   "source": [
    "### üóΩ Image + Timestamp + Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b768f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = 'https://upload.wikimedia.org/wikipedia/commons/6/63/Times_Square%2C_New_York_City_%28HDR%29.jpg'\n",
    "response = requests.get(image_url)\n",
    "image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "image.show()\n",
    "\n",
    "contextual_note = 'Visited Times Square, lots of people, bright screens ‚Äî 7:45pm, Jan 15'\n",
    "print(f\"Note: {contextual_note}\")\n",
    "\n",
    "# Get embeddings\n",
    "inputs_image = clip_processor(images=image, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    image_features = clip_model.get_image_features(**inputs_image)\n",
    "\n",
    "inputs_text = clip_processor(text=[contextual_note], return_tensors=\"pt\", padding=True)\n",
    "with torch.no_grad():\n",
    "    text_features = clip_model.get_text_features(**inputs_text)\n",
    "\n",
    "# Normalize & fuse\n",
    "image_embedding = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "text_embedding = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\n",
    "fused_embedding = (image_embedding + text_embedding) / 2\n",
    "\n",
    "ngm.add_node(\n",
    "    node_id=\"memory_times_square_evening\",\n",
    "    modality=\"image+text\",\n",
    "    content=contextual_note,\n",
    "    embedding=fused_embedding.squeeze().tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39b7bb1",
   "metadata": {},
   "source": [
    "### üîÅ Diagram + Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721ad895",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/10/Transformer.png/600px-Transformer.png'\n",
    "response = requests.get(image_url)\n",
    "image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "image.show()\n",
    "\n",
    "interpretation = 'This shows the self-attention mechanism used in GPT-based models.'\n",
    "print(f\"Interpretation: {interpretation}\")\n",
    "\n",
    "# Embed both\n",
    "inputs_image = clip_processor(images=image, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    image_features = clip_model.get_image_features(**inputs_image)\n",
    "\n",
    "inputs_text = clip_processor(text=[interpretation], return_tensors=\"pt\", padding=True)\n",
    "with torch.no_grad():\n",
    "    text_features = clip_model.get_text_features(**inputs_text)\n",
    "\n",
    "image_embedding = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "text_embedding = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\n",
    "fused_embedding = (image_embedding + text_embedding) / 2\n",
    "\n",
    "ngm.add_node(\n",
    "    node_id=\"memory_transformer_diagram\",\n",
    "    modality=\"image+text\",\n",
    "    content=interpretation,\n",
    "    embedding=fused_embedding.squeeze().tolist()\n",
    ")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
