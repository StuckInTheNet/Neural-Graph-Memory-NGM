{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88d13286",
   "metadata": {},
   "source": [
    "# Neural Graph Memory: Demo \n",
    "This notebook demonstrates:\n",
    "- Text & image memory insertion\n",
    "- Temporal & multimodal graph structure\n",
    "- Associative retrieval\n",
    "- Graph traversal\n",
    "- Persistence (save/load)\n",
    "- Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92ce8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision transformers sentence-transformers networkx matplotlib Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9aabbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630535b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryNode:\n",
    "    def __init__(self, id, embedding, modality, metadata=None):\n",
    "        self.id = id\n",
    "        self.embedding = embedding\n",
    "        self.modality = modality\n",
    "        self.metadata = metadata or {}\n",
    "\n",
    "class NeuralGraphMemory:\n",
    "    def __init__(self):\n",
    "        self.graph = nx.Graph()\n",
    "        self.text_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.image_encoder = models.resnet18(pretrained=True)\n",
    "        self.image_encoder.fc = torch.nn.Identity()\n",
    "        self.image_encoder.eval()\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize(256), T.CenterCrop(224),\n",
    "            T.ToTensor(), T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                      std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def add_text_node(self, text):\n",
    "        embedding = self.text_encoder.encode(text)\n",
    "        return self._add_node(embedding, 'text', {\"text\": text})\n",
    "\n",
    "    def add_image_node(self, img_url):\n",
    "        image = Image.open(BytesIO(requests.get(img_url).content)).convert(\"RGB\")\n",
    "        tensor = self.transform(image).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            embedding = self.image_encoder(tensor).squeeze().numpy()\n",
    "        return self._add_node(embedding, 'image', {\"url\": img_url})\n",
    "\n",
    "    def _add_node(self, embedding, modality, metadata):\n",
    "        node_id = len(self.graph)\n",
    "        node = MemoryNode(node_id, embedding, modality, metadata)\n",
    "        self.graph.add_node(node_id, data=node)\n",
    "        if node_id > 0:\n",
    "            self.graph.add_edge(node_id - 1, node_id, relation=\"temporal\")\n",
    "        return node_id\n",
    "\n",
    "    def retrieve_nearest(self, query, top_k=3):\n",
    "        query_emb = self.text_encoder.encode(query)\n",
    "        similarities = []\n",
    "        for nid, data in self.graph.nodes(data=\"data\"):\n",
    "            sim = np.dot(data.embedding, query_emb) / (\n",
    "                np.linalg.norm(data.embedding) * np.linalg.norm(query_emb))\n",
    "            similarities.append((nid, sim))\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_k]\n",
    "\n",
    "    def traverse_temporal(self, start_id=0):\n",
    "        return list(nx.dfs_preorder_nodes(self.graph, source=start_id))\n",
    "\n",
    "    def visualize(self):\n",
    "        labels = nx.get_node_attributes(self.graph, \"data\")\n",
    "        pos = nx.spring_layout(self.graph)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        nx.draw(self.graph, pos, with_labels=True, node_color='lightblue')\n",
    "        node_texts = {k: (v.metadata.get(\"text\") or \"img\")[:15] for k, v in labels.items()}\n",
    "        nx.draw_networkx_labels(self.graph, pos, labels=node_texts, font_size=8)\n",
    "        plt.title(\"Neural Graph Memory\")\n",
    "        plt.show()\n",
    "\n",
    "    def save(self, path='ngm.pkl'):\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self.graph, f)\n",
    "\n",
    "    def load(self, path='ngm.pkl'):\n",
    "        with open(path, 'rb') as f:\n",
    "            self.graph = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417e9b1f",
   "metadata": {},
   "source": [
    "## Memory Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d1be3f",
   "metadata": {},
   "source": [
    "### Adding a Visual Memory Node\n",
    "We now demonstrate how an AI agent might ingest a visual memory (e.g., a memory architecture diagram) into the graph memory system using CLIP embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c64a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "image_url = 'https://raw.githubusercontent.com/StuckInTheNet/neural-graph-memory/main/assets/architecture.png'\n",
    "response = requests.get(image_url)\n",
    "image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b43ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngm.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2666c3",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6bf96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What happened with the dog?\"\n",
    "nearest = ngm.retrieve_nearest(query)\n",
    "print(f\"Query: {query}\\n\\nTop Matches:\")\n",
    "for nid, score in nearest:\n",
    "    node = ngm.graph.nodes[nid][\"data\"]\n",
    "    print(f\" - Node {nid} ({node.modality}): {node.metadata} [Score: {score:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b814130",
   "metadata": {},
   "source": [
    "## Temporal Traversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3519be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = ngm.traverse_temporal()\n",
    "print(\"Traversal order:\")\n",
    "for nid in order:\n",
    "    node = ngm.graph.nodes[nid][\"data\"]\n",
    "    print(f\" - Node {nid}: {node.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2cf56f",
   "metadata": {},
   "source": [
    "## Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6240e630",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngm.save(\"ngm_demo.pkl\")\n",
    "print(\" Saved memory graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631369d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngm2 = NeuralGraphMemory()\n",
    "ngm2.load(\"ngm_demo.pkl\")\n",
    "print(\"Loaded memory graph with\", len(ngm2.graph), \"nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da9e323",
   "metadata": {},
   "source": [
    "## Multimodal Memory Demonstrations\n",
    "This section simulates how an AI agent stores, embeds, and retrieves rich multimodal memories.\n",
    "Each example below demonstrates a different modality or modality combination processed into graph memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fb9148",
   "metadata": {},
   "source": [
    "### Image + Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7a7c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "image_url = 'https://upload.wikimedia.org/wikipedia/commons/9/91/Golden_Retriever_Carlos_%2810581910556%29.jpg'\n",
    "response = requests.get(image_url)\n",
    "image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "image.show()\n",
    "\n",
    "caption = 'Saw a golden retriever chasing a ball in Central Park.'\n",
    "print(f\"Caption: {caption}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91a0f45",
   "metadata": {},
   "source": [
    "### Text + Audio (Simulated Voice Note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4309a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated transcription of a voice note\n",
    "voice_note = 'Remember to review the memory architecture notes after today’s meeting.'\n",
    "print(f\"Transcribed voice note: {voice_note}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90df0578",
   "metadata": {},
   "source": [
    "### Image + Timestamp + Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137d499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = 'https://upload.wikimedia.org/wikipedia/commons/6/63/Times_Square%2C_New_York_City_%28HDR%29.jpg'\n",
    "response = requests.get(image_url)\n",
    "image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "image.show()\n",
    "\n",
    "contextual_note = 'Visited Times Square, lots of people, bright screens — 7:45pm, Jan 15'\n",
    "print(f\"Note: {contextual_note}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bafa45",
   "metadata": {},
   "source": [
    "###  Diagram + Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4438a4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/10/Transformer.png/600px-Transformer.png'\n",
    "response = requests.get(image_url)\n",
    "image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "image.show()\n",
    "\n",
    "interpretation = 'This shows the self-attention mechanism used in GPT-based models.'\n",
    "print(f\"Interpretation: {interpretation}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
